{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "create and implement a basic neural model within a computational framework. integrate essential elements like input node, weight parameters, bias and a activation function (including but not limited to sigmoid, hyperbolic tangent and relu function) to construct a compresensive representation oof a neuron, evaluate  and observe how each acitvation function inlfuences the networks behaviour and effectiveness in handelling different types of data\n",
        "\n"
      ],
      "metadata": {
        "id": "OYRgttv6bKc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return max(0,x)\n",
        "\n",
        "def leaky_relu(x):\n",
        "    return max(0.1*x,x)\n",
        "\n",
        "\n",
        "\n",
        "def activate(input,weights):\n",
        "    h = 0\n",
        "    for x,w in zip(inputs,weights):\n",
        "        h+= x*w\n",
        "    return {\"Sigmoid\":sigmoid(h),\"Tanh\":tanh(h),\"Relu\":relu(h),\"Leaky relu\":leaky_relu(h)}\n",
        "\n",
        "inputs = [0.5, 0.3, 0.2]\n",
        "weights = [0.4, 0.7, 0.2]\n",
        "\n",
        "output = activate(inputs,weights)\n",
        "print(\"Outputs of different activation function is :\")\n",
        "for activate, out in output.items():\n",
        "    print(activate,\"-->\",out)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80TXcivEcVnm",
        "outputId": "b651261f-3427-4500-dcc2-7dfd092d2aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs of different activation function is :\n",
            "Sigmoid --> 0.389360766050778\n",
            "Tanh --> 0.42189900525000795\n",
            "Relu --> 0.45000000000000007\n",
            "Leaky relu --> 0.45000000000000007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take training inputs which is an np.array"
      ],
      "metadata": {
        "id": "Pt31n1X4j6rO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OiH82w5ApAbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}